# Kubernetes Persistent Volumes

I have been really busy this month, hence not much materialized in terms of blog posts. Anyway, I have taking a short break and in this time I have experimented further with persistent volumes, using my [previous blog post](2022-05-07.md) as a base.

The ide is to add a persistent volume to a Pod. As a test, I want to use a GlusterFS volume that is also mounted on my local host OS. In theory anything I can see or do in the Pod should also be visible on my local OS mount point.

## K3s Caveats

It should be important to note that K3s is a _lightweight_ Kubernetes distribution and [excludes](https://rancher.com/docs/k3s/latest/en/storage/) certain plugins, like GlusterFS.

If you follow a typical [Kubernetes GlusterFS example](https://github.com/kubernetes/examples/tree/master/volumes/glusterfs), you will notice the following output when you run something like `kubectl describe pod/glusterfs`:

```text
Name:         glusterfs
Namespace:    pvtest
Priority:     0
Node:         node2/10.0.50.222
Start Time:   Tue, 24 May 2022 10:44:26 +0200
Labels:       <none>
Annotations:  <none>
Status:       Pending
IP:           
IPs:          <none>
Containers:
  glusterfs:
    Container ID:   
    Image:          nicc777/demo-flask-app
    Image ID:       
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /mnt/glusterfs from glusterfsvol (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mkjv6 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  glusterfsvol:
    Type:           Glusterfs (a Glusterfs mount on the host that shares a pod's lifetime)
    EndpointsName:  glusterfs-cluster
    Path:           volume1
    ReadOnly:       true
  kube-api-access-mkjv6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age                From               Message
  ----     ------       ----               ----               -------
  Normal   Scheduled    92s                default-scheduler  Successfully assigned pvtest/glusterfs to node2
  Warning  FailedMount  34s (x2 over 49s)  kubelet            Unable to attach or mount volumes: unmounted volumes=[glusterfsvol], unattached volumes=[kube-api-access-mkjv6 glusterfsvol]: failed to get Plugin from volumeSpec for volume "glusterfsvol" err=no volume plugin matched
  Warning  FailedMount  3s (x5 over 92s)   kubelet            Unable to attach or mount volumes: unmounted volumes=[glusterfsvol], unattached volumes=[glusterfsvol kube-api-access-mkjv6]: failed to get Plugin from volumeSpec for volume "glusterfsvol" err=no volume plugin matched
```

Since the plugin is not available, the Pod will never start or get into a ready state. I still plan to test the example in another environment at some point in time - perhaps that is also for another blog post... Time will tell.

According to the [K3s documentation](https://rancher.com/docs/k3s/latest/en/storage/) there are two options:

* Using a local storage provider, or
* Using Longhorn.

There is a nice [Longhorn on K3s for Raspberry Pi](https://rpi4cluster.com/k3s/k3s-storage-setting/) that will be great to follow, but I decided to first try a local storage provider.

## Lab Setup

The lab implementation will be based on K3s exposing persistent volumes using a local storage provider.

Below is a simplified diagram of what I'm trying to accomplish. The diagram itself is based on the [C4 model deployment diagram](https://c4model.com/#DeploymentDiagram) example.

![setup](../../images/blog_2022_05_25/k3s_persistent_volumes.drawio.png)

# Preparing the cluster for the local storage provider

Before deploying a pod, some preparation work is required. The nest couple of steps will delve into the detail of the preparations of the k3s cluster.

## Preparing Nodes

As a pre-requisite I have my GlusterFS, as described in [my previous blog](2022-05-07.md), up and running.

The biggest task really was to prepare the GlusterFS setup on each node. I have [blogged previously](2022-04-03.md) about setting up k3s using multipass. I am using that as base and I have created an additional script ([available here](https://gist.githubusercontent.com/nicc777/164d5c65510d369248fa50c338a71ba7/raw/2297b5725d4b64bd9428d73375f4f3d1f3a7654e/install_glusterfs_clinet_on_k3s_nodes.sh)) to easily install the required packages and mount the GlusterFS volume.

To setup and mount the volume in the k3s nodes, simply run `bash install_glusterfs_clinet_on_k3s_nodes.sh`.

_**Note**_: In the next step it will become clear why the script creates the mount point on `/opt/local-path-provisioner`.

## Installing the local storage provider

To setup the local storage provider, I followed [this README](https://github.com/rancher/local-path-provisioner) and used the stable build, which was version 0.0.22 at the time of experimenting.

The installation command was run as it was on the README:

```shell
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.22/deploy/local-path-storage.yaml
```

Verify installation with the command `kubectl get all -n local-path-storage`:

```text
NAME                                          READY   STATUS    RESTARTS   AGE
pod/local-path-provisioner-7c795b5576-nklqr   1/1     Running   0          54s

NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/local-path-provisioner   1/1     1            1           54s

NAME                                                DESIRED   CURRENT   READY   AGE
replicaset.apps/local-path-provisioner-7c795b5576   1         1         1       54s
```

The configuration is defined in a ConfigMap which expects the local storage to be at `/opt/local-path-provisioner`. This is why the previous step mounted the GlusterFS volume on this mount point.

# Tags

file-systems, kubernetes, persistent volumes, pv

<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://nicc777.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
