# Exploring Kubernetes from within Kubernetes... with Python

So my blog posts will obviously not be daily, as I start to delve into deeper more technical topics.

Today I'm looking at using Python and integrating with the Kubernetes API.

To make this practical, I choose a topic for which there may be a number of better alternatives, but yet, it's good to gain understanding on some of these mechanisms.

I want to know one simple thing: Do I have enough physical resources (CPU and RAM) to handle all my committed deployments?

For this post, I also assume the reader to be familiar with some basics of Kubernetes. Feel free to refer to [this blog entry](2022-04-03.md) that shows how I implement my lab environments and on which commands in this blog post will also be based.

## Context

Certain services, like [AWS EKS](https://aws.amazon.com/eks/) provide out-of-the-box solutions to automatically scale the number of nodes. The scaling metrics can be tweaked, but for the most part it all just happens automatically.

However... Sometimes you run Kubernetes on physical tin or in a cloud environments with some financial constraints, and you have to ensure that the deployed applications do not consume more than the available resources. This can be tricky, but one of the fundamental problems to solve upfront is to know how much capacity is available and then measure what is actually being committed. Actual usage may be lower than the total commitment, however, but for now we focus on what is committed as this serves as some upper boundary or guarantee that our users expect to be honored if their applications run on our cluster.

## Background on resources

I am going to assume anyone reading this will at least understand the concepts around CPU and RAM - particularly around the physical constraints and at least some basic understanding of how application use CPU and RAM resources.

Over and above this basic understanding There are also important principles to take note of:

| Principle                      | Description                                                                                                                                                                             |
|--------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Resources are finite           | As mentioned, in our context of physical hardware, both CPU and RAM resources are finite - we cannot use more than what is available                                                    |
| Swap is not an option          | There is much debate around using swap within the Kubernetes ecosystem, but for the purpose of this blog we assume no swap mechanism is being used (standard for the bulk of use cases) |
| Deployments must impose limits | This is a best practice principle. Refer to the [Kubernetes documentation](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) for the gory detail.         |

If we look at our [Python Demonstration/Reference Application](https://github.com/nicc777/pyk8sdemo-app), the following section if the Kubernetes manifest is important:

```yaml
        resources:
          requests:
            memory: "128Mi"
            cpu: "250m"
          limits:
            memory: "256Mi"
            cpu: "500m"
```

The specifics of what this means and technically how this is implemented, can be [read in this section](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-requests-are-scheduled) of the Kubernetes documentation.

What is important is that although you could get away with over-subscribing the CPU (within reasonable limits), memory (RAM) is far less forgiving. Without any swap available, should all your pods start to consume too much available RAM, Kubernetes will start to evict pods. The technical detail of this subject and how it is implemented is vast, but feel free to refer to the Kubernetes documentation dealing with [Scheduling, Preemption and Eviction](https://kubernetes.io/docs/concepts/scheduling-eviction/).

_**Note**_: Since [my last post](2022-04-03.md) I have updated the demo application to include a memory monster :-D



